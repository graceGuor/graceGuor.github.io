<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[wct]]></title>
    <url>%2F2019%2F04%2F23%2Fwct%2F</url>
    <content type="text"><![CDATA[本文学习任意风格只需要训练一个WCT模型，而且训练过程中不需要风格图的风格转换论文&quot;Universal Style Transfer via Feature Transforms&quot;。 作者 Code Authors:Yijun Li, Zhaowen Wang, Chen Fang, Xin Lu, Jimei Yang, Ming-Hsuan Yang 2017 NIPS Abstract 目前的基于前馈的方法，推断时很有效，但是对于未见过的风格无法有效进行推断，或者在图片质量上妥协。我们提出一种简单有效，不需要在预先定义的风格图上训练的方法。关键的组成部分是被用于图片重建网络的一对特征转换，whitening和coloring。 whitening 和coloring 转换反映了内容图和给定风格图特征协方差的直接匹配，这和优化Gram矩阵来进行风格转换有相似的思想。 Introduction 主要挑战是如何提取风格图的有效表示，然后将其匹配到内容图中。 Gram矩阵和协方差矩阵可以很好的刻画特征之间的关系，所以被用来捕获视觉风格特征。 基于优化的方法可以处理任意风格的图片，并得到令人满意的结果，但是计算量巨大。基于前馈的方法可以很快得到转换后的图片，但是只能应用于固定数量的风格，或者在图片质量上妥协。 在每一个中间层，我们的目标是转换提取的内容特征，使得它们和相同层的风格特征有一样的统计特性。我们发现在这些特征上做经典的WCT转换可以实现这一目标。 我们首先利用VGG-19进行特征提取（encoder），然后训练一个对称decoder来反转VGG-19特征到原始图片，本质上就是重建图片。一旦训练完成，encoder和decoder网络在整个实验中就固定不动了。 为了进行风格转换，我们对内容特征的某一层应用WCT，使得其协方差矩阵和风格特征的协方差矩阵匹配。转换后 的特征作为decoder层的输入以获得风格化后的图片。除了单层风格化，还有多层风格化pipeline，其效果比单层的好。 当给一个新的风格图时，我们不需要对风格图在已有的风格基础上进行微调，而是提取特征协方差矩阵，然后将它们通过WCT应用到内容特征上。 本文主要算法结构如下： Proposed algorithm 我们将风格转换表示为用特征转换（如WCT）进行图片重建的过程。 reconstruction decoder decoder被设计为与VGG-19对称的 loss为： $$ L = ||I_o - I_i|| _2 ^2 + \lambda || \Phi (I_o) - \Phi (I_i) || _2 ^2 $$ whitening and coloring transforms 本节用到的一些符号解释 符号 含义 $ I_c $ 内容图 $ I_s $ 风格图 $ f_c \in R ^{ C × H_c W_c } $ 内容图的VGG特征， $ H_c $ 和 $ W_c $ 分别代表内容特征的高和宽， $ C $ 是channel数 $ f_s \in R ^{ C × H_s W_s } $ 风格图的VGG特征 $ \hat f_c $ 白化特征 将 $ f_c $ 直接喂给decoder，decoder将重构原始图片 $ I_c $ 。接下来我们将使用WCT相对 $ f_s $ 的统计调整 $ f_c $ 。WCT的目标是直接转化 $ f_c $ 以匹配 $ f_s $ 的协方差矩阵。包含两步：whitening transform和coloring transform。 Whitening transform 在whitening之前，首先通过减去 $ f_c $ 的均向量 $ m_c $ 来集中 $ f_c $ ，然后将 $ f_c $ 进行如下线性转换得到 $ \hat f_c $ 使得特征之间是不相关（即正交矩阵）的（ $ \hat f_c \hat f_c^T = I $ ，转置=逆 的矩阵为正交矩阵）。 $$ \hat f_c = E_c D _c ^{ - \frac {1}{2} } E_c^T f_c $$ $ D_c $ 是协方差矩阵 $ f_c f_c^T \in R ^{C × C} $ 特征值的对角矩阵， $ E_c $ 是对应特征向量的正交矩阵，满足 $ f_c f_c^T = E_c D_c E_c^T $ 。 解读上式 协方差矩阵的 $-\frac {1}{2} $ 方*矩阵=每行均变为均值为0，方差为1的向量 $ f_c f_c^T $ 为协方差矩阵，求其 $ - \frac {1} {2} $ 方，不太好求得，所以对其进行对角线分解，对得到的对角矩阵进行分解，得到 $ f_c f_c^T = E_c D_c E_c^T $ ，其中 $ (E_c D_c E_c^T) ^{-\frac {1}{2}} = E_c D_c ^{ - \frac {1}{2} } E_c^T $ 。 为了验证白化特征 $ \hat f_c $ 中编码了哪些信息，我们用我们之前训练好的decoder将其反转到RGB空间。图2显示白化之后的特征依然保留了图内容的全局结构，但是移除了其他与风格相关的信息。 Coloring transform 首先减去 $ f_s $ 的均向量 $ m_s $ 来集中 $ f_s $ ，然后执行coloring 转换，即对 $ \hat f_c $ 进行线性变换，其本质上是whitening步的逆步骤。这样，我们就能得到特征之间有着需要关系的 $ \hat f_{cs} $ （ $ \hat f_{cs} \hat f_{cs} ^T = f_s f_s^T $ ）。 $$ \hat f _{ cs } = E _s D _s ^{ \frac { 1 } { 2 } } E _s ^T \hat f _c $$ $ D_s $ 是协方差矩阵 $ f_s f _s ^T \in R ^{ C × C } $ 特征值的对角矩阵， $ E_s $ 是对应特征向量的正交矩阵。 最后，给 $ \hat f _{ cs } $ 加上均向量 $ m_s $ 进行去中心化， $ \hat f _{ cs } = \hat f _{cs} + m_s $ 经过WCT变换后，在将其输入到decoder之前，将 $ \hat f _{ cs } $ 和 $ \hat f _{ c } $ 进行线性组合，方便用户控制风格化效果的程度。 $$ \hat f _{ cs } = \alpha \hat f _{cs} + ( 1- \alpha ) f_c $$ $ \alpha $ 为风格权重。 multi-level coarse-to-fine stylization coarse-to-fine风格化中，高层特征捕捉风格的显著特征，低层特征增强细节。如果换成fine-to-coarse，低层信息在经过高层特征之后无法保留。 Experimental results decoder training dataset：MSCOCO style transfer DeepArt：基于优化，可以处理任意的风格，但很可能遇到局部极小值问题 TNet：风格化速度提高了，但是为了效果，其在质量和通用性之间做了权衡，使得其会产生相同内容覆盖的重复模块 style-swap：能力有限，当风格在低阶信息（颜色）中很难反映时，内容特征会被严格保留 Huang：内容特征仅仅用来被调整以获得和风格特征相同的均值和方差，对于获得风格图的高阶表示是无效的。对于未见过的风格图没有通用性。对于捕获并合成显著风格模式是无效的，尤其是有着丰富局部特征和非平滑区域的复杂的风格。 我们的方法：没有学习任何风格，我们的模型可以捕获风格图中明显的外观模式。此外，内容图中的主要部分在我们的模型中可以被风格化，其他方法只能转化相对平滑的区域。 用协方差矩阵来定量分析风格化图片和风格图的差异 user study efficiency 在WCT中有一个特征值分解步，但随着图片size的增加，这一步的计算开销并不会随之增大，因为协方差矩阵的维度只取决于filter channels的数量，最大也就是512（Relu_5_1）。 user control 不仅仅能进行风格转换，还能满足用户的不同风格化需求，如： 1. 大小：调整风格图输入的size 2. 权重：风格图和内容图占比，不需要重新进行优化，直接计算 3. 空间控制：内容图的不同部分用不同风格进行转换，需要输入额外的masks M texture synthesis 通过将内容图设置为随机噪声图（如高斯噪声），我们的风格化框架可以将其应用为纹理合成，或者可以直接将 $ \hat f_c $ 初始化为白噪声，两种方法能得到相同的结果。 经验结果：运行几次多层pipeline可以得到令人视觉上满意的结果。 我们的方法也能将两种纹理进行结合，生成新的纹理效果。 可生成多样性的纹理合成结果 参考 Universal Style Transfer via Feature Transforms]]></content>
      <categories>
        <category>style transfer</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
        <tag>WCT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[style_swap]]></title>
    <url>%2F2019%2F04%2F17%2Fstyle-swap%2F</url>
    <content type="text"><![CDATA[本文学习任意风格只需要训练一个style-swap模型的风格转换论文&quot;Fast Patch-based Style Transfer of Arbitrary Style&quot;。 这是Torch Code 作者源码 Authors ：Tian Qi Chen ，Mark Schmidt publication in 2016 Abstract 现有问题 1. 对所有风格图均有效，但很贵 2. 或者只对有限的被训练过的风格图有效 本文工作 本文提出基于本地匹配的一种简单的优化目标：在预训练网络的某一层结合内容结构和风格纹理 我们的目标有令人满意的属性如简单的优化景观、直觉参数调整和在视频中一致的逐帧性能。 程序对任意内容和风格图都适用 Introduction 一个经典的加速加速解决方案：训练另一个神经网络，该神经网络近似于单个前馈神经网络传递优化中的最优值。然而，目前的工作牺牲了通用性，因为前馈神经网络无法推广到训练集中没有出现过的图片。 因此，目前的应用，要不就耗时比较长，要不就只能提供有限风格的转化。 本文提出一个仅仅依赖于CNN某一层的新的优化目标，目前的方法使用CNN中的多层。新的目标函数使得我们使用“inverse network”确切地在风格化层逆转激活函数，然后产生风格化之后的图片。 Related work Style Transfer as Optimization 速度慢 Feed-forward Style Networks 对于每一种新的风格图需要重新训练 Style Transfer for Video Inverting Deep Representations 为了与已有风格转化方法进行对比，我们提出一种方法，直接重构预训练好的CNN网络中某一层的目标激活函数。 像Li和Wand一样，我们使用一个标准在激活函数空间寻找最佳匹配patch。但是我们的方法直接构建整个激活目标。这个确切的过程使得我们的方法很容易应用到视频中，而不用考虑一致性问题。 我们不用像素级别的loss，而是在激活函数上使用loss。 在特定的训练设置下，逆转网络甚至可以逆转一般CNN激活函数范围之外的激活函数 A new objective for style transfer 同样基于预训练好的VGG网络，在某一ReLU激活层进行基于patch的交换操作：将内容图片的每一个patch用与其最接近的风格patch代替。 本文主要算法结构如下： Style swap 本节用到的一些符号解释 符号 含义 $ C $ 内容图的RGB表示 $ S $ 风格图的RGB表示 $ \Phi( \cdot ) $ 将一幅图用预训练好的VGG网络从RGB表示转换为某些中间激活层的转换函数 $ \Phi( C ) $ 内容图的激活层表示 $ \Phi( S ) $ 风格图的激活层表示 $ \phi _i ( C ) $ 内容图在位置$ i $的激活函数patch $ \phi _i ( S ) $ 风格图在位置$ i $的激活函数patch $ \phi _{a,b} ( C ) $ 内容在位置$ a,b $的激活函数patch：$ \Phi ( C ) _{a:a+s,b:b+s,1:d} $,其中$ s $代表patch size $ \phi _i ^{SS} ( C,S ) $ 与内容图激活patch$ i $最接近的风格patch $ \Phi ^{SS} ( C,S ) $ 重构的完整内容图激活层 交换过程如下 对内容图和风格图分别提取patch，分别用$ { \phi _i ( C ) } _{i \in n_c} $ $ { \phi _i ( S ) } _{i \in n_s} $表示，其中$ n_c $ $ n_s $为提取的patch。提取的patch应该有足够多的重叠，并包含所有激活函数channel 对于每个内容激活patch，基于归一化互相关度量方法，得到与其最接近的风格patch $$ \phi _i ^{SS} ( C,S ) := \mathop {\arg \max} _{\phi _j ( S ), j=1, \cdots , n_s} {\dfrac {&lt;\phi _i ( C ), \phi _j ( S )&gt;} {||\phi _i ( C )|| \cdot ||\phi _j ( S )||} } \text{ (1) } $$ 将每个内容激活patch $ \phi _i ( C ) $，用与其最接近的风格patch$ \phi _i ^{SS} ( C,S ) $替换 重构完整的内容图激活层，用$ \Phi ^{SS} ( C,S ) $表示，重叠部分取平均值 这样就能得到一个既有内容图的结构，又有风格图的纹理的图片的隐藏激活层。 parallelizable implementation 因为对于每一个内容激活函数patch的argmax操作，其归一化项是个常数，所以式(1)可以重写为 $$ K _{a,b,j} = &lt;\phi _{a,b} ( C ), \frac {\phi _j ( S )} {||\phi _j ( S )|| } &gt; $$ $$ \phi _{a,b} ^{SS} ( C,S ) = \mathop {\arg \max} _{ \phi _j ( S ),j \in N_s } { { K _{a,b,j} } } $$ 内容激活函数patch归一化项的缺失可简化计算，且使得我们可以直接使用2D卷积层。 实现步骤 2D卷积层：$ K $ 可以用归一化后的风格激活函数patch $ \left( \dfrac { \phi _j ( S ) } { || \phi _j ( S ) || } \right) $ 作为卷积层， $ \Phi ( C ) $ 作为输入进行卷积操作得到。计算结果 $ K $ 有 $ n_c $ 个空间位置， $ n_s $ 个feature channels。在每个空间位置， $ K _{ a,b } $为内容激活函数patch和所有的风格激活函数patch之间的互相关向量。 channel-wise的argmax：将$ K_{a,b} $用一个one-hot向量代替 $$ \overline{K} _{a,b,j} = \begin{cases} 1, &amp; \text{if $j = \mathop {\arg \max} _{ j^{'} { K _{a,b,j^{'}} } } $ } \ 0, &amp; \text{otherwise} \end{cases} $$ 2D转置卷积层：将$ \overline{K} $作为输入，未归一化的风格激活函数patchs $ { \phi _j ( S ) } $作为filter进行转置卷积操作。在每个空间位置，只有最匹配的风格激活函数patch在输出中，其他的patch乘以0。 optimization formulation 合成图表示可以通过在目标激活函数$ \Phi ^{SS} ( C,S ) $上的loss来计算得到。 $$ I _{stylized} ( C,S ) = \mathop { \arg \min } _{ I \in R ^{ h×w×d } } { || \Phi (I) - \Phi ^{ SS } ( C,S ) || _F ^2 + \lambda l _{TV} (I) } $$ 合成的图的size为$ h×w×d $，$ || \cdot || _F $ 代表Frobenius范数，$ l _{TV} (\cdot) $ 代表total variation（全变分）正则项。 Inverse network inverse network的主要目的是对于任意目标激活函数，近似损失函数的最优值，因此我们定义优化逆转函数为 $$ \mathop { \arg \min } _{ f } E _H \left [ || \Phi ( f(H) ) - H || _F ^2 + \lambda l _{ TV } ( f(H) ) \right] $$ $ H $ 代表目标激活函数，$ f $ 代表逆转神经网络。 Training the Inverse Network 由于预训练好的卷积网络，有两个问题 非单射：CNN中的卷积层、maxpooling、和ReLU层都是多对一的，因此没有合适的逆运算。类似于已有的工作，我们不训练逆转关系，而是训练一个参数化的神经网络 $$ \min _{ \theta } \dfrac { 1 } { n } \sum _{ i=1 } ^n \left [ || \Phi ( f( H_i ; \theta ) ) - H_i || _F ^2 + \lambda l _{ TV } ( f( H_i ; \theta ) ) \right ] $$ $ \theta $ 代表逆转神经网络 $ f $ 的参数， $ H_i $ 代表有 $ n $ 个样本的数据集中的激活函数特征。 非满射：如果逆转网络仅仅只训练了真实图片，那么逆转网络可能只能逆转在$ \Phi( \cdot ) $范围内的激活函数。鉴于我们希望逆转风格交换后的激活函数，所以我们增强训练数据集以包含这些激活函数值。 Feedforward Style Transfer Procedure 风格转换过程 计算$ \Phi( C ) $和$ \Phi( S ) $ 通过风格交换获得$ \Phi ^{SS} ( C,S ) $ 将$ \Phi ^{SS} ( C,S ) $喂入训练好的逆转网络 Experiments Style Swap Results 目标层为relu3_1是效果最好 训练更少轮便能到达最优点 随机初始化对结果影响不大 比其他现有目标函数少很多局部极小值 一致性：可以直接应用到视频中，而不用任何类似光流的粘合 CNN Inversion $ \lambda = 10 ^{-6} $ Adam learning_rate:$ 10 ^ {-3} $ 数据增强的效果好于未增强的效果 Computation Time 时间大多花费在风格交换步 图片越大，花费时间越长 参考 Fast Patch-based Style Transfer of Arbitrary Style]]></content>
      <categories>
        <category>style transfer</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
        <tag>style_swap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优化算法]]></title>
    <url>%2F2019%2F04%2F11%2FOptimizer%2F</url>
    <content type="text"><![CDATA[本文介绍机器学习中不同优化算法之间的差异。 大概步骤 首先用一个框架来梳理所有的优化算法 定义 待优化参数： $ w $ 目标函数： $ f(w) $ 初始学习率 $ \alpha $ 进行迭代优化 在每个epoch $ t $ ，进行如下步骤： 计算目标函数关于当前参数的梯度： $ g_t=\nabla f(w_t) $ 根据历史梯度计算一阶动量和二阶动量：$ m_t = \phi(g_1, g_2, \cdots, g_t) $;$ V_t = \psi(g_1, g_2, \cdots, g_t) $ 计算当前时刻的下降梯度： $ \eta_t = \dfrac {\alpha}{\sqrt{V_t}} \cdot m_t $ 根据下降梯度进行更新： $ w_{t+1} = w_t - \eta_t $ 步骤3、4对于各个算法都是一致的，主要的差别就体现在1和2上。 比较 优化算法 区别 优点 缺点 $ \mathrm{SGD} $ 没有动量的概念 $ \eta_t = \alpha \cdot g_t $ 1. 下降速度慢 2.可能会在沟壑的两边持续震荡，停留在一个局部最优点 $ \mathrm{SGDM(SGD with Momentum)} $ 在SGD基础上引入了一阶动量： $ m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot g_t $ $ \beta_1 $的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。 一阶动量是各个时刻梯度方向的指数移动平均值（EMA），约等于最近 $ 1/(1-\beta_1) $ 个时刻的梯度向量和的平均值。 为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些 $ \mathrm{NAG(Nesterov Accelerated Gradient)}$ 在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向： $ g_t=\nabla f(w_t - \dfrac { \alpha }{ \sqrt{V_{t-1}} } \cdot m_{t-1}) $ 然后用下一个点的梯度方向，与历史累积动量相结合，计算步骤2中当前时刻的累积动量。 $ \mathrm{AdaGrad} $ SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。 怎么样去度量历史更新频率呢？那就是二阶动量该维度上，迄今为止所有梯度值的平方和： $ V_t = \sum_{\tau=1}^{t} g_\tau^2 $ $ \eta_t = \dfrac {\alpha}{\sqrt{V_t}} \cdot g_t $ 在稀疏数据场景下表现非常好 因为$ \sqrt{V_t} $ 是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识 $ \mathrm{AdaDelta} $ 主要修改了第3步 learning rate是自己算出来的 $ E_t = \gamma \cdot E_{t-1} + (1-\gamma) \eta_t^2 $ $ \eta_t = \dfrac { \sqrt{ E _{t-1} } } { \sqrt{V_t} } \cdot g_t $ 避免了二阶动量持续累积、导致训练过程提前结束的问题了 $ \mathrm{RMSProp} $ 由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略： 不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。 这也就是AdaDelta名称中Delta的来历。 修改的思路很简单。前面我们讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量： $ V_t = \beta_2 * V_{t-1} + (1-\beta_2) g_t^2 $ $ \eta_t = \dfrac {\alpha}{\sqrt{V_t}} \cdot g_t $ 避免了二阶动量持续累积、导致训练过程提前结束的问题了 $ \mathrm{Adam} $ 把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum $ m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot g_t $ $ V_t = \beta_2 * V_{t-1} + (1-\beta_2) g_t^2 $ 学习速度快 泛化能力不好 $ \mathrm{Nadam} $ Nesterov + Adam = Nadam $ g_t=\nabla f(w_t - \dfrac { \alpha }{ \sqrt{V_{t-1}} } \cdot m_{t-1}) $ 指数移动平均值的偏差修正 前面我们讲到，一阶动量和二阶动量都是按照指数移动平均值进行计算的： $ m_t = \beta_1 \cdot m_{t-1} + (1-\beta_1)\cdot g_t $ $ V_t = \beta_2 \cdot V_{t-1} + (1-\beta_2) \cdot g_t^2 $ 实际使用过程中，参数的经验值是:$ \beta_1=0.9 $, $ \beta_2=0.999 $ 初始化：$ m_0=0 $, $ V_0=0 $ 这个时候我们看到，在初期， $ m_t $, $ V_t $ 都会接近于0，这个估计是有问题的。因此我们常常根据下式进行误差修正： $ \tilde{m}_t = \dfrac {m_t}{1-\beta_1^t} $ , $ \tilde{V}_t = \dfrac {V_t}{1-\beta_2^t} $ NAG与普通momentum的区别 Tip 一般在CV（computer Vision）中用SGD+Momentum比自适应算法效果好，部分文章中提到的Momentum默认为Nesterov momentum 参考 一个框架看懂优化算法之异同 SGD/AdaGrad/Adam An overview of gradient descent optimization algorithms NAG与普通momentum区别图]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Optimizer</tag>
        <tag>优化</tag>
        <tag>SGD</tag>
        <tag>Momentum</tag>
        <tag>NAG</tag>
        <tag>Adam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Gatys]]></title>
    <url>%2F2019%2F04%2F05%2FGatys%2F</url>
    <content type="text"><![CDATA[本文学习风格转换（style transfer）祖师爷Gatys的论文&quot;Image Style Transfer Using Convolutional Neural Network&quot;和&quot;A Neural Algorithm of Artistic Style&quot;。 Abstract 之前的工作无法明确地表示语义信息，因此无法将图片内容和风格区分开来。我们使用为目标检测训练出的CNN特征表示来明确地表示高层图片信息。 Deep image representations 使用归一化19层VGG网络提供的特征空间。不用任何全连接层，用average pooling比max pooling效果好。 Content representation 内容loss在内容图和生成图之间计算： $$ L_{content}(\overrightarrow p, \overrightarrow x, l) = \frac {1}{2} \sum (F ^l _{ij} - P ^l _{ij} ) ^2 $$ Style representation 分别计算生成图和风格图中间层的Gram矩阵$ A ^l _{ij} $和$ G ^l _{ij} $作为它们的风格表示，其中$ G ^l _{ij} $的计算方法如下： $$ G ^l _{ij} = \sum _k F ^l _{ ik } F ^l _{ jk } $$ 那么生成图和风格图之间的总风格loss为： $$ E_l = \frac {1} { 4 N_l^2 M_l^2 } \sum _{ i,j } ( G ^l _{ij} - A ^l _{ij} ) ^2 $$ $$ L _{style} ( \overrightarrow a, \overrightarrow x ) = \sum _{ l=0 } ^L w_l E_l $$ Style transfer 我们需要最小化如下loss： $$ L _{total} ( \overrightarrow p , \overrightarrow a , \overrightarrow x ) = \alpha L _{content} ( \overrightarrow p, \overrightarrow x ) + \beta L _{style} ( \overrightarrow a, \overrightarrow x ) $$ 其算法结构如下： 在图片合成中，L-BFGS优化效果最好 为了在可比较的范围内提取图片信息，在特征表示计算之前，经常会把风格图片的size缩放到和内容图片一样 没有用image priors进行规范化 image priors：图片中相邻的像素可能会比较相似，所以部分损失函数中会加上image priors这一项：如相邻的像素点差值的平方和 Results 实验结果如下所示，其中内容用了conv4_2层，风格用了conv1_1, conv2_1, conv3_1, conv4_1, conv5_1层 Initialisation of gradient descent 白噪声图、内容图、风格图初始化的结果相差不是很大 Discussion 训练速度与图片分辨率线性相关 有时会受限于低阶噪声，艺术风格转化是问题不是很大，但当内容图和风格图都是照片时会是一个比较严重的问题 将图片内容和风格分开表示并不是一个定义的很好的问题 参考 Image Style Transfer Using Convolutional Neural Network A Neural Algorithm of Artistic Style]]></content>
      <categories>
        <category>style transfer</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
        <tag>Gatys</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[style transfer概述]]></title>
    <url>%2F2019%2F03%2F27%2Fstyle-transfer%2F</url>
    <content type="text"><![CDATA[本文对风格转换（style transfer）进行概述。 简介 什么是图像风格化（Neural Style Transfer）？ 图像风格化是将一张照片渲染成有艺术风格的画作。图像风格化算法的输入有二，分别是内容图和风格图，输出有一个，为风格迁移后的结果图。 上面这张表示图像风格化算法将输入的一张长城的照片作为内容图，将富春山居图作为风格图，将长城的内容保留，风格替换成中国山水画的风格，创作出一张新的艺术作品。 问题 如何对风格图中的风格特征进行建模和提取？ 纹理可以用图像局部特征的统计模型来描述。风格可以看成一种纹理，这部分可以借鉴纹理建模方法的相关研究 成功把风格图中的风格抽出来后，如何和内容混合然后还原成一个相应的风格化结果呢？ 图像重建 下图是图像风格化算法的起源和基石 算法 如上所说，图像风格化迁移算法=纹理建模算法+图像重建算法 到目前为止，这些算法可以按照如下规则分类 基于在线图像优化的慢速图像风格化迁移算法（Slow Neural Method Based On Online Image Optimisation） 基于统计分布的参数化慢速风格化迁移算法（Parametric Slow Neural Method with Summary Statistics） 通过名字就可以看出，这一类风格化算法是由基于在线图像优化的慢速图像重建方法和基于统计分布的参数化纹理建模方法结合而来。 图像风格化迁移这一领域的祖师爷 Gatys的开山大作就是属于这一类方法的。 其中，对于纹理建模，Gram矩阵是最常用的损失函数 基于MRF的非参数化慢速风格化迁移算法（Non-parametric Slow Neural Method with MRFs） 提出了一个取代 Gram 损失的新的 MRF 损失。思路与传统的 MRF 非参数化纹理建模方法相似，即先将风格图和重建风格化结果图分成若干 patch，然后对于每个重建结果图中的 patch，去寻找并逼近与其最接近的风格 patch。 基于离线模型优化的快速图像风格化迁移算法（Fast Neural Method Based On Offline Model Optimisation） 前面介绍的都是用慢速图像重建方法对风格化结果进行重建的，所以速度肯定是比较慢的，而且很吃资源，在工业界落地的成本肯定是很高的。 所以另外一个大的图像风格化迁移算法分支——快速图像风格化迁移算法主要解决速度问题，核心思想就是利用基于离线模型优化的快速图像重建方法对风格化结果进行重建，基预先训练前向网络来解决计算量大、速度慢的问题。 根据一个训练好的前向网络能够学习到多少个风格作为分类依据，我们将快速图像风格化迁移算法分为单模型单风格（PSPM）、单模型多风格（MSPM）和单模型任意风格（ASPM）的快速风格化迁移算法。 不同快速风格化方法所需时间对比图： 单模型单风格的快速风格化迁移算法（Per-Style-Per-Model Fast Neural Method,PSPM） 针对每一个风格图，我们去训练一个特定（style specific）的前向模型，这样当测试的时候，我们只需要向前向模型扔进去一张内容图，就可以前向出一个风格化结果了。 这一类算法（简称PSPM）其实可以再分成两类：（1）一类是基于统计分布的参数化快速风格化PSPM算法以及（2）基于MRF的非参数化PSPM算法。 基于残差网络的设计能更好地最小化风格化损失函数。 单模型多风格的快速风格化迁移算法（Multiple-Style-Per-Model Fast Neural Method,MSPM） 把多个风格整合到一个模型中，理论上是合理的。比如咱们就说中国山水画，有很多著名山水画作品，但不同山水画虽然风格不尽相同，但是还是有很多相似的地方的（相似特征），所以对每一幅山水画训练得到的网络之间理论上是有共享的部分的。 沿着这个思路想，我们发掘出不同风格网络之间共享的部分，然后对于新的风格只去改变其有差别的部分，共享的部分保持不变。 基于此思想，有以下工作： Google Brain ：在训练好的一个风格化网络基础上，只通过在 Instance Norlization 层上做一个仿射变换（他们起了个名字叫 Conditional Instance Normalization，简称 CIN） 陈冬冬 ：StyleBank: An Explicit Representation for Neural Image Style Transfer Amazon AI 的 张航 ：在 2017 年 3 月提出 Multi-style Generative Network for Real-time Transfer：只用一个网络，把通过 VGG 网络提取到的风格特征与风格化网络中的多个尺度的中间层的 feature map 通过提出的 Inspiration Layer 结合在一起，相当于将风格特征作为信号输入到网络中来决定要风格化成哪一个风格。 浙大李一君：Diversified Texture Synthesis With Feed-Forward Networks。除了把风格特征作为信号外，另一个选择是把图像像素作为信号输入进去风格化网络。 李一君学长首先将每一张风格图与一个随机产生的噪声图进行绑定，然后将噪声图与风格化网络中间层的 feature map 进行 concat，作为网络进行风格选择的信号。 单模型任意风格的快速风格化迁移算法（Arbitrary-Style-Per-Model Fast Neural Method，ASPM） Zero-shot Fast Style Transfer，即来一个新风格不需要训练，我们就可以很快速地把风格化结果输出来（这里我们称之为单模型任意风格，简称ASPM）。 多伦多大学的 Tian Qi Chen（不是 UW 的明星博士生陈天奇）：Fast Patch-based Style Transfer of Arbitrary Style。这个算法是基于 patch 的，可以归到基于 MRF 的非参数化 ASPM 算法。在 CNN 特征空间中，找到与内容 patch 匹配的风格 patch 后，进行内容 patch 与风格 patch 的交换（Style Swap），之后用快速图像重建算法的思想对交换得到的 feature map 进行快速重建。但由于 style swap 需要一定的时间开销，没有达到实时。 康奈尔的大牛 @Xun Huang ： Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization。其工作主要受到 MSPM 的 CIN 层启发，提出一个 Adaptive Instance Normalization (AdaIN)。AdaIN 的输入是通过 VGG 提取的风格和内容特征，用数据驱动的方式，通过在大规模风格和内容图上进行训练，让 AdaIN 能够直接将图像中的内容 normalise 成不同的风格。 Google Brain ：Exploring the Structure of a Real-time, Arbitrary Neural Artistic Stylization Network，数据驱动。 李一君： Universal Style Transfer via Feature Transforms。用一种不需要学习训练的方式（style learning-free），而是单纯使用一系列特征变换来进行 ASPM 风格迁移。 参考 综述：图像风格化算法最全盘点 | 内附大量扩展应用]]></content>
      <categories>
        <category>style transfer</category>
      </categories>
      <tags>
        <tag>style transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语言模型]]></title>
    <url>%2F2019%2F03%2F24%2F%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86C5%2F</url>
    <content type="text"><![CDATA[语言模型（language model，LM）目前主要采用的是n元语法模型（n-gram model），这种模型构建简单、直接，但同时也因为数据缺乏而必须采取平滑（smoothing）算法。 本章主要介绍n元语法的基本概念和几种常用的数据平滑方法。 n元语法 一个语言模型通常构建为字符串 $ s$ 的概率分布 $ p(s)$ ，这里 $ p(s)$ 试图反映的是字符串 $ s$ 作为一个句子出现的频率。 对于一个由 $ l$ 个基元（“基元”可以为字、词或短语等，为了表述方便，之后统一用“词”来通指）构成的句子 $ s = w_1w_2 \cdots w_l$ ，其概率计算公式可以表示为： $$ p(s) = p(w_1)p(w_2|w_1)p(w_3|w_1w_2) \cdots p(w_l|w_1 \cdots w_{l - 1}) = \prod ^{l} _{i = 1} {p(w_i | w_1 \cdots w _{i-1} )} $$ 式中，产生第 $ i$ 个词的概率是由已经产生的 $ i - 1$ 个词 $ w_1w_2 \cdots w_{i - 1}$ 决定的。 如果直接这么算的话，自由参数太多，我们几乎不可能从训练数据中正确地估计出这些参数，而且实际上，绝大多数历史根本就不可能在训练数据中出现。因此，为了解决这个问题，可以将历史 $ w_1w_2 \cdots w_{i - 1}$ 按照某个法则映射到等价类 $ E(w_1w_2 \cdots w_{i - 1})$ ，而等价类的数目远远小于不同历史的数目。如将两个历史 $ w_{i - n + 2} \cdots w_{i - 1}w_i$ 和 $ v_{k - n + 2} \cdots v_{k - 1}v_k$ 映射到同一个等价类，当 $ n = 2$ 时，即出现在第 $ i$ 位上的词 $ w_i$ 仅与它前面的一个历史词 $ w_{i-1}$ 有关，二元文法模型被称为一阶马尔可夫链（Markov chain），记作bigram或bi-gram。 以bigram为例，如果用 $ c(w_{i-1}w_i)$ 表示二元语法 $ w_{i-1}w_i$ 在给定文本中的出现次数，则条件概率 $ p(w_i|w_{i-1})$ 可以用如下公式计算： $$ p(w_i|w_{i-1}) = \frac {c(w_{i-1}w_i)} { \sum _{w_i} {c(w _{i-1} w_i)} } $$ 对于n元语法模型，使用的训练语料的规模一般要有几百万个词。上式用于估计 $ p(w_i|w_{i-1})$ 的方法称为 $ p(w_i|w_{i-1})$ 的最大似然估计（maximum likelihood estimation，MLE）。 语言模型性能评价 交叉熵和困惑度越小越好。 在英语文本中，n元语法模型计算的困惑度大约为 $50 \sim 1000 $ 之间（对应的交叉熵范围为 $6\sim 10$ 个比特位），具体值与文本的类型有关。 数据平滑 问题的提出 “数据平滑”是用来解决零概率问题的。“平滑”处理的基本思想是“劫富济贫”，即提高低概率（如零概率），降低高概率，尽量使概率分布趋于均匀。 加法平滑方法 假设每一个n元词法发生的次数比实际统计次数多 $ \delta $ 次， $ 0 \leq \delta \leq 1$ ，则 $$ p _{add} (w_i|w ^{i-1} _{i-n+1}) = \frac { \delta + c(w _{i-n+1} ^i)} { \delta |V| + \sum _{w_i} {c(w _{i-n+1} ^i)}} $$ 古德-图灵（Good-Turing）估计法 Good-Turing估计法是很多平滑技术的核心。其基本思路是：对于任何一个出现 $ r$ 次n元语法，都假设它出现了 $ r^* $ 次，这里 $$ r^* = (r+1) \frac {n_{r+1}}{n_r} $$ 其中 $ n_r$ 是训练预料中恰好出现 $ r$ 次的 $ n$ 元语法的数目。要把这个统计次数转化为概率，需要进行归一化处理：对于统计数为 $ r$ 的 $ n$ 元语法，其概率为 $$ p_r = \frac { r^* }{N} $$ 其中 $ N = \sum _{r=0} ^{\infty} {n_r r^* } $ 。注意： $$ N = \sum _{r=0} ^{ \infty } {n_r r^* } = \sum _{r=0} ^{ \infty } { (r+1) { n _{r+1}} } = \sum _{r=1} ^{ \infty } {n_rr} $$ 也就是说， $ N$ 等于这个分布中最初的计数。这样，样本中所有事件的概率之和为： $$ \sum _{r&gt;0} {n_r p_r} = \sum _{r&gt;0} { \frac {(r+1) n _{r+1} } {N}} = \sum _{r&gt;1} { \frac {rn_r} {N} } = 1 - \frac {n_1} {N} &lt; 1 $$ 因此，有 $ \frac {n_1}{N}$ 的概率剩余量可以分配给所有未见过的事件（ $ r=0$ 的事件）。 Good-Turing方法不能实现高阶模型和低阶模型的结合，而高低阶模型的结合通常是获得较好的平滑效果所必须的。通常，Good-Turing方法作为一个基本方法，在其他平滑技术中得到了很好的应用。 Katz平滑方法 1987年S. M. Katz提出了一种后备（back-off）平滑方法，简称Katz平滑方法。其基本思路是，当事件在样本中出现的频次大于某一数值 $ k$ 时，运用最大似然估计法，通过减值来估计其概率值；而当事件的频次小于 $ k$ 时，使用低阶的语法模型作为代替高阶语法模型的后备，但这种代替受归一化因子的约束。换句话说，就是将因减值而节省下来的概率根据低阶语法模型的分布情况分配给未见事件，而不是平均分配。 以bigram为例，对于一个出现次数为 $ r=c(w^i_{i-1})$ 的二元语法 $ w^i_{i-1}$ ，使用如下公式计算修正的计数： $$ p_{katz}(w^i_{i-1}) = \begin {cases} d_r \dfrac {c(w_{i-1}^i)} { c(w_{i-1}) }, &amp; \text {r&gt;0} \\ \alpha (w_{i-1}) p_{ML}(w_i), &amp; \text{r=0} \end {cases} $$ 也就是说，所有具有非零计数 $ r$ 的二元语法都根据折扣 $ d_r$ 被减值了，折扣率 $ d_r $ 近似地等于 $ \dfrac {r_*}r $ ，这个减值是由Good-Turing估计方法预测的。式中， $ p_{ML}(w_i)$ 为 $ w_i$ 的最大似然估计概率， $ \alpha (w_{i-1})$ 为一个合适的值。 折扣率 $ d_r$ 可以按照如下办法计算：由于大的计数值是可靠的，因此它们不需要进行减值。尤其对于某些 $ k$ ,S. M. Katz取所有 $ r&gt;k$ 的情况下的 $ d_r = 1$ ，并且建议 $ k=5$ 。对于 $ r \leq k$ 的情况下的折扣率，减值率由用于全局二元语法的Good-Turing估计方法计算。 Jelinek-Mercer平滑方法 以bigram为例，将二元文法模型和一元文法模型进行线性插值： $$ p_{interp}(w_i|w_{i-1}) = \lambda p_{ML}(w_i|w_{i-1}) + (1- \lambda )p_{ML}(w_i) $$ 其中， $ 0 \leq \lambda \leq 1 $ 。 对于多元语法模型，第 $ n$ 阶平滑模型递归地定义为 $ n$ 阶最大似然估计模型和 $ n-1$ 阶平滑模型之间的线性插值： $$ p_{interp}(w_i|w ^{i-1} _{i-n+1}) = \lambda _{w _{i-n+1} ^{i-1}} p _{ML} (w_i|w ^{i-1} _{i-n+1}) + (1- \lambda _{w _{i-n+1} ^{i-1}}) p _{interp} (w ^{i-1} _{i-n+2}) $$ 绝对减值法 绝对减值法（absolute discounting）类似于Jelinek-Mercer平滑算法，涉及高阶和低阶模型的插值问题。然而，这种方法不是采用将高阶最大似然分布乘以因子 $ \lambda w_{i-n+1}^{i-1} $ 的方法，而是通过从每个非零计数中减去一个固定值 $ D \leq 1 $ 的方法来建立高阶分布。 $$ p _{abs} (w_i|w ^{i-1} _{i-n+1}) = \frac {\max c(w _{i-n+1} ^{i}) - D, 0} { \sum _{w_i} c(w _{i-n+1} ^{i})} + (1- \lambda _{w _{i-n+1} ^{i-1}}) p _{abs} (w ^{i-1} _{i-n+2}) $$ 这里假设 $ 0 \leq D \leq 1 $ 。 Kneser-Ney平滑方法 这是一种扩展的绝对减值算法，用一种新的方式建立与高阶分布相结合的低阶分布。在前面的算法中，通常用平滑后的低阶最大似然分布作为低阶分布，在Kneser-Ney平滑方法中，分配给（以bigram为例）每个一元文法计数的数目就是它前面不同单词的数目。 修正的Kneser-Ney平滑方法 不像Kneser-Ney平滑方法那样对所有的非零计数都用一个减值参数 $ D$ ，而是对计数分别为1、2和大于等于3三种情况下的 $ n$ 元模型分别采用3个不同的参数 $ D_1$ 、 $ D_2$ 、 $ D_{3+}$ 。 平滑方法的比较 不管训练语料规模多大，对于二元语法和三元语法而言，Kneser-Ney平滑方法和修正的Kneser-Ney平滑方法的效果都好于其他所有的平滑方法。 语言模型自适应方法 在实际应用中，语言模型对跨领域的脆弱性和独立性假设的无效性是两个最明显的问题。 为了提高语言模型对语料的领域、主题、类型等因素的适应性，提出了以下自适应语言模型。 基于缓存的语言模型 基于缓存的自适应模型针对的问题是，在文本中刚刚出现过的一些词在后边的句子中再次出现的可能性往往更大，比标准的 $ n$ 元语法模型预测的概率要大。 cache-based自适应方法的基本思路是：语言模型通过 $ n$ 元语法的线性插值求得： $$ \hat p( w_i|w _1 ^{i-1} ) = \lambda \hat p _{Cache} ( w_i|w _1 ^{i-1} ) + ( 1 - \lambda ) \hat p _{n-gram} (w_i|w _{i-n+1} ^{i-1}) $$ 常用的方法是：在缓存中保留前面的 $ K $ 个单词，每个词 $ w_i $ 的概率（缓存概率）用其在缓存中出现的相对频率计算得出： $$ \hat p _{Cache} (w_i|w_1 ^{i-1}) = \frac {1}{K} \sum _{j=i-K} ^{i-1} {I _{w_j = w_i} } $$ 其中， $ I _{\epsilon} $ 为指示器函数（indicator function）。如果 $ \epsilon $ 表示的情况出现，则 $ I _{\epsilon} = 1 $ ，否则， $ I _{\epsilon}=0 $ 。 研究表明，缓存中每个词对当前词的影响随着与该词距离的增大呈指数级衰减，因此上式可写为： $$ \hat p _{Cache} (w_i|w_1 ^{i-1}) = \beta \sum _{j=1} ^{i-1} {I _{w_j = w_i} } e ^{- \alpha (i-j)} $$ 其中， $ \alpha$ 为衰减率， $ \beta$ 为归一化系数。 基于混合方法的语言模型 针对的问题是：由于大规模训练语料本身是异源的，来自不同领域的语料存在一定的差异，而测试语料一般是同源的，因此，为了获得最佳性能，语言模型必须适应各种不同类型的语料对其性能的影响。 基于混合方法的自适应语言模型的基本思想是：将语言模型划分成 $ n$ 个子模型 $ M_1,M_2, \cdots, M_n$ ，整个语言模型的概率通过下式计算得到： $$ \hat p(w_i|w_1 ^{i-1}) = \sum _{j=1} ^n \lambda _j \hat p _{M_j} (w_i|w_1 ^{i-1}) $$ 基于最大熵的语言模型 上面介绍的两种语言模型自适应方法都是分别建立各个子模型，然后将子模型的输出结合起来。基于最大熵的语言模型却采用不同的实现思路，即通过结合不同信息源的信息构建一个语言模型。每个信息源提供一组关于模型参数的约束条件，在所有满足约束的模型中，选择熵最大的模型。 参考 宗成庆《统计自然语言处理》第五章]]></content>
      <categories>
        <category>统计自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>语言模型</tag>
        <tag>数据平滑</tag>
        <tag>n-gram</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[语料库与语言知识库]]></title>
    <url>%2F2019%2F03%2F22%2F%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86C4%2F</url>
    <content type="text"><![CDATA[本章介绍两个语料库，英文语料库WordNet和中文语料库HowNet。 WordNet WordNet是美国普林斯顿大学认知科学实验室George A. Miller领导的研究组开发的英语机读词汇知识库。 WordNet按照语义关系网络组织，多种词汇关系和语义关系被用来表示词汇知识的组织方式。词形式（word form）和词义（word meaning）是WordNet源文件中可见的两个基本构件，词形式以规范的词形表示，词义以同义词集合（synset）表示。词汇关系是两个词形式之间的关系，而语义关系是两个词义之间的关系。 WordNet的建立有三个基本前提： 1.“可分离性假设（separability hypothesis）”：语言的词汇成分可以被离析处理并专门针对它加以研究。 2.“模式假设（pattern hypothesis）”：一个人不可能掌握他运用一种语言所需的所有词汇，除非他能够利用词义中存在的系统的模式和词义之间的关系。 3.“广泛性假设（comprehensiveness hypothesis）”：计算语言学如果希望能像人那样处理自然语言，就需要像人那样储存尽可能多的词汇知识。 知网（HowNet） 知网（HowNet）是机器翻译专家董振东和董强建立的语言知识库，是一个以汉语和英语的词语所代表的概念为描述对象，以揭示概念与概念之间以及概念所具有的属性之间的关系为基本内容的常识知识库。 知网体系的基本设想是：所有的概念都可以分解成各种各样的义原（最基本的、不易于再分割其意义的最小单位），同时，也存在一个有限的义原集合，其中的义原组合成一个无限的概念集合。 常识性知识库是知网最基本的数据库，又称为知识词典。 知网是在线的，其规模是动态的，它的规模主要取决于双语知识词典数据文件的大小。 知网的知识词典主要为那些具有多个义项的词提供了使用例子。这些例子的要求是：强调例子的区别能力而不是它们的释义能力，它们的用途在于为消除歧义提供可靠的帮助。 参考 宗成庆《统计自然语言处理》第四章]]></content>
      <categories>
        <category>统计自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>语料库</tag>
        <tag>WordNet</tag>
        <tag>HowNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息论基本概念]]></title>
    <url>%2F2019%2F03%2F17%2F%E7%BB%9F%E8%AE%A1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86C2%2F</url>
    <content type="text"><![CDATA[本章介绍信息论的一些基本概念，如熵、联合熵、条件熵、互信息、相对熵、交叉熵、困惑度，以及噪声信道模型。 熵 如果X是一个离散型随机变量，取值空间为R，其概率分布为$ p(x) = P(X = x),x \in R$，那么X的熵（entropy） $ H(X)$定义为： $$ H(X) = - \sum_{x \in R} {p(x) \log _2p(x)} $$ 其中，约定$ 0 \log 0 = 0$。$ H(X)$可以写为$ H(p)$。之后将$ \log _2p(x)$简写成$ \log p(x)$。 熵又称为自信息（self-information），可以视为描述一个随机变量所需的信息量 $ \log (x)$代表编码x需要的长度 为什么熵最大的模型是最符合事件真实分布的模型 最大熵：整个系统是用无信息先验模型来进行拟合，现有数据是一些evidence，根据这些evidence不断更新先验知识，组成新的先验知识对新的数据进行模拟，除了已有的evidence之外，所有情况出现的概率应该是一致的，这种情况下熵最大，我们没有办法假设有些情况出现的概率更高，所以使得熵最大的模型能最真实的反应事件的分布情况 联合熵和条件熵 如果X，Y是一对离散型随机变量 $X,Y \sim p(x,y)$，X，Y的联合熵（joint entropy）$ H(X,Y)$定义为： $$ H(X,Y) = - \sum_{x \in X}\sum_{y \in Y} {p(x,y) \log p(x,y)} $$ 给定随机变量X的情况下，随机变量Y的条件熵（conditional entropy） 定义为： $$ H(Y|X) = \sum_{x \in X}{p(x)H(Y|X=x)} = \sum_{x \in X}{p(x)} \left[\sum_{y \in Y}p(y|x) \log (y|x) \right] = -\sum_{x \in X}\sum_{y \in Y} {p(x,y) \log p(y|x)} $$ 熵的连锁规则： $$ H(X_1,X_2,\cdots,X_n) = H(X_1) + H(X_2|X_1) + \cdots + H(X_n|X_1,\cdots,X_{n - 1}) $$ 一般地，对于一条长度为n的信息，每一个字符或字的熵为： $$ H_{rate} = \frac{1}{n} H(X_{1n}) = - \frac{1}{n} \sum_{x_{1n}}{p(x_{1n}) \log p(x_{1n})} $$ 这个数值称为熵率（entropy rate）。其中，变量 $ X_{1n}$ 表示随机变量序列$ (X_1,X_2,\cdots,X_n)$，$ x_{1n} = (x_1,x_2,\cdots,x_n)$。 假定一种语言是由一系列符号组成的随机过程，$ L(X_i)$，例如，某报纸的一批语料，那么，我们可以定义这种语言L的熵作为其随机过程的熵率，即 $$ H_{rate}(L) = \lim_{n \to \infty} \frac{1}{n} H(X_1,X_2,\cdots,X_n) $$ 互信息 根据熵的连锁规则，有 $$ H(X,Y) = H(X) + H(Y|X) = H(Y) + H(X|Y) $$ 因此， $$ H(X) - H(X|Y) = H(Y) - H(Y|X) $$ 这个差叫X和Y的互信息（mutual information），记作$ I(X;Y)$。或者定义为：如果 $X,Y \sim p(x,y)$，则X，Y之间的互信息$ I(X;Y) = H(X) - H(X|Y)$。 $I(X;Y)$反映的是在知道了Y的值之后X的不确定性的减少量。可以理解为Y的值透露了多少关于X的信息量。 由于$ H(X|X) = 0$，因此 $$ H(X) = H(X) - H(X|X) = I(X;Y) $$ 这一方面说明了为什么熵又叫自信息，另一方面说明了两个互相依赖的变量之间的互信息不是一个常量，而是取决于它们的熵。 相对熵 相对熵（relative entropy） 又称Kullback-Leibler差异（Kullback-Leibler divergence），或简称KL距离，是衡量相同事件空间里两个概率分布相对差距的测度。两个概率分布$ p(x)$和$ q(x)$的相对熵定义为： $$ D(p||q) = \sum_{x \in X}{p(x) \log {\frac{p(x)}{q(x)}}} $$ 该定义中约定$ 0 \log (0/q) = 0$，$ q \log (q/0) = \infty$，表示成期望值为： $$ D(p||q) = E_p \left( \log {\frac{p(X)}{q(X)}} \right) $$ $ \log (p(x) / q(x))$ 表示用分布p来编码需要的长度 - 用q来编码所需长度 显然，当两个随机分布完全相同时，即$p = q$，其相对熵为0。 互信息实际上就是衡量一个联合分布与独立性差距多大的测度： $$ I(X;Y) = D(p(x,y)||p(x)p(y)) $$ 交叉熵 如果一个随机变量X ~ $ p(x)$，$ q(x)$为用于近似$ p(x)$的概率分布，那么，随机变量X和模型$ q$之间的 交叉熵（cross entropy） 定义为： $$ H(X,q) = H(X) + D(p||q) = H(X,p) + D(p||q) = - \sum_{x} {p(x) \log q(x)} = E_p \left( \log {\frac{1}{q(x)}} \right) $$ 由此，可以定义语言 $L= (X) \sim p(x)$与其模型$ q$的交叉熵为： $$ H(L,q) \approx -\frac{1}{N} \log {q(x_1^N)} $$ 马尔可夫链：第n个词出现的概率只与 1 ... n-1 个词有关 稳态过程：第n个词出现的概率只与前n-1个词有关，与时间状态无关，比如句子：&quot;我 有 ... ,，我 有 ...&quot;，第一个“有”出现的概率为1/100，第二个“有”出现的概率也是1/100，与这个词出现在句首、句中还是句尾无关。 困惑度 给定语言L的样本$ l_1^n = l_1,l_2,\cdots,l_n$，L的困惑度（perplexity） $PP_q$定义为： $$ PP_q = 2^{H(L,q)} \approx 2^{-\frac{1}{n} \log q(l_1^n)} = \left [ q(l_1^n) \right ]^{-\frac{1}{n}} $$ 语言模型的任务就是寻找困惑度最小的模型，使其最接近真实语言的情况。 噪声信道模型 香农（Claude Elwood Shannon）提出了噪声信道模型（noisy channel model），其目标就是优化噪声信道中信号传输的吞吐量和准确率，其基本假设是一个信道的输出以一定的概率依赖于输入。 信道容量（capacity） 的定义如下，其基本思想是用降低传输速率来换取高保真通信的可能性。 $$ C = \max_{p(X)}I(X;Y) $$ 根据这个定义，如果能够设计一个输入编码X，其概率分布为$ p(X)$，使其输入与输出之间的互信息达到最大值，那么，我们的设计就达到了信道的最大传输容量。从数学上讲，信道容量C就是平均互信息量的最大值。 参考 宗成庆《统计自然语言处理》第二章]]></content>
      <categories>
        <category>统计自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>信息论</tag>
        <tag>熵</tag>
      </tags>
  </entry>
</search>
